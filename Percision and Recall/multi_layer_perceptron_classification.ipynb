{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mosaMLP import MLP\n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1257, 64), (1257, 10), (540, 64), (540, 10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "Y = np.eye(10)[Y]  # one hot\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 0.3349964975557618 acc train: 0.1288782816229117\n",
      "loss test: 0.3145453009096426 acc test: 0.12222222222222222\n",
      "loss train: 0.2996322574354489 acc train: 0.23389021479713604\n",
      "loss test: 0.29448821019084415 acc test: 0.2518518518518518\n",
      "loss train: 0.28326435290198865 acc train: 0.3221957040572792\n",
      "loss test: 0.28301287609071407 acc test: 0.32037037037037036\n",
      "loss train: 0.2711251194153281 acc train: 0.4081145584725537\n",
      "loss test: 0.272920100840897 acc test: 0.40185185185185185\n",
      "loss train: 0.2602620053594599 acc train: 0.4805091487669053\n",
      "loss test: 0.2646250815244694 acc test: 0.45\n",
      "loss train: 0.24949537385706513 acc train: 0.5465393794749404\n",
      "loss test: 0.2563483165296616 acc test: 0.5074074074074074\n",
      "loss train: 0.23920350428664788 acc train: 0.6054097056483692\n",
      "loss test: 0.2479577016265888 acc test: 0.55\n",
      "loss train: 0.22957559997176782 acc train: 0.6523468575974543\n",
      "loss test: 0.24000667657157013 acc test: 0.5796296296296296\n",
      "loss train: 0.22030735868187598 acc train: 0.6992840095465394\n",
      "loss test: 0.23283051125395246 acc test: 0.6203703703703703\n",
      "loss train: 0.2111914993016023 acc train: 0.7271280827366746\n",
      "loss test: 0.22624293566476172 acc test: 0.6444444444444445\n",
      "loss train: 0.20309005131885927 acc train: 0.7533810660302307\n",
      "loss test: 0.22057680990654355 acc test: 0.6648148148148149\n",
      "loss train: 0.19595455871058665 acc train: 0.7788385043754972\n",
      "loss test: 0.2152058108860489 acc test: 0.6833333333333333\n",
      "loss train: 0.18958106383692244 acc train: 0.7979315831344471\n",
      "loss test: 0.2104963972489515 acc test: 0.7018518518518518\n",
      "loss train: 0.18370131520651387 acc train: 0.8082736674622116\n",
      "loss test: 0.20636369083372982 acc test: 0.7166666666666667\n",
      "loss train: 0.17833841378065943 acc train: 0.8194112967382657\n",
      "loss test: 0.20277258661029043 acc test: 0.7240740740740741\n",
      "loss train: 0.17337855907615463 acc train: 0.8321400159108989\n",
      "loss test: 0.19954066609154536 acc test: 0.725925925925926\n",
      "loss train: 0.16869386413898574 acc train: 0.8448687350835322\n",
      "loss test: 0.1964768469328779 acc test: 0.7351851851851852\n",
      "loss train: 0.16433821431614112 acc train: 0.8536197295147175\n",
      "loss test: 0.1933839579491405 acc test: 0.7481481481481481\n",
      "loss train: 0.1603379119119552 acc train: 0.8568019093078759\n",
      "loss test: 0.19081071945053127 acc test: 0.7518518518518519\n",
      "loss train: 0.15650198128311296 acc train: 0.8647573587907716\n",
      "loss test: 0.18844982789342474 acc test: 0.7611111111111111\n",
      "loss train: 0.15294055672749335 acc train: 0.86793953858393\n",
      "loss test: 0.18626225394632673 acc test: 0.7703703703703704\n",
      "loss train: 0.14959989268654264 acc train: 0.8750994431185362\n",
      "loss test: 0.18402976174115918 acc test: 0.7759259259259259\n",
      "loss train: 0.1464475111906854 acc train: 0.8798727128082736\n",
      "loss test: 0.18186826406420148 acc test: 0.7777777777777778\n",
      "loss train: 0.14341178876364327 acc train: 0.8870326173428799\n",
      "loss test: 0.17968678661159238 acc test: 0.7833333333333333\n",
      "loss train: 0.1405020723968115 acc train: 0.8965791567223548\n",
      "loss test: 0.1774951535737175 acc test: 0.7870370370370371\n",
      "loss train: 0.13771508589318296 acc train: 0.9013524264120922\n",
      "loss test: 0.1754047032194836 acc test: 0.7962962962962963\n",
      "loss train: 0.1351145159415153 acc train: 0.9061256961018298\n",
      "loss test: 0.17346136715948218 acc test: 0.8055555555555556\n",
      "loss train: 0.13264811098209428 acc train: 0.9085123309466985\n",
      "loss test: 0.1717069127691754 acc test: 0.8074074074074075\n",
      "loss train: 0.1302874820512046 acc train: 0.9124900556881463\n",
      "loss test: 0.17016786307222753 acc test: 0.8074074074074075\n",
      "loss train: 0.12799358228822202 acc train: 0.9180588703261734\n",
      "loss test: 0.1686613247369512 acc test: 0.8092592592592592\n",
      "loss train: 0.12569439929393147 acc train: 0.9172633253778838\n",
      "loss test: 0.1671584503856274 acc test: 0.8111111111111111\n",
      "loss train: 0.12344317548272948 acc train: 0.9180588703261734\n",
      "loss test: 0.16568822229735394 acc test: 0.8148148148148148\n",
      "loss train: 0.12130491808368854 acc train: 0.9228321400159109\n",
      "loss test: 0.1642750767706494 acc test: 0.8203703703703704\n",
      "loss train: 0.11920010971154876 acc train: 0.9284009546539379\n",
      "loss test: 0.16291838228673697 acc test: 0.8240740740740741\n",
      "loss train: 0.11715229190764946 acc train: 0.9307875894988067\n",
      "loss test: 0.16160458966417016 acc test: 0.8240740740740741\n",
      "loss train: 0.1152181980389303 acc train: 0.9331742243436754\n",
      "loss test: 0.16040200029183002 acc test: 0.8277777777777777\n",
      "loss train: 0.11338209570781736 acc train: 0.9379474940334129\n",
      "loss test: 0.15926924208193602 acc test: 0.8314814814814815\n",
      "loss train: 0.11163246137876275 acc train: 0.9403341288782816\n",
      "loss test: 0.15817185628477615 acc test: 0.8333333333333334\n",
      "loss train: 0.10995809843001393 acc train: 0.94351630867144\n",
      "loss test: 0.15711652357540187 acc test: 0.8333333333333334\n",
      "loss train: 0.10832389693955546 acc train: 0.9466984884645983\n",
      "loss test: 0.15610600306519254 acc test: 0.8333333333333334\n",
      "loss train: 0.10670700320238528 acc train: 0.9474940334128878\n",
      "loss test: 0.15514620126616308 acc test: 0.837037037037037\n",
      "loss train: 0.10511715132370858 acc train: 0.9482895783611774\n",
      "loss test: 0.15423798631576857 acc test: 0.8388888888888889\n",
      "loss train: 0.10357958178321786 acc train: 0.949085123309467\n",
      "loss test: 0.15338280154683936 acc test: 0.8388888888888889\n",
      "loss train: 0.10211035444933754 acc train: 0.9522673031026253\n",
      "loss test: 0.1525766832860523 acc test: 0.8407407407407408\n",
      "loss train: 0.10070010117065253 acc train: 0.954653937947494\n",
      "loss test: 0.15180579366975697 acc test: 0.8425925925925926\n",
      "loss train: 0.09933758400664952 acc train: 0.9554494828957836\n",
      "loss test: 0.1510653810126116 acc test: 0.8444444444444444\n",
      "loss train: 0.09801383135588806 acc train: 0.9562450278440732\n",
      "loss test: 0.15035521821630082 acc test: 0.8444444444444444\n",
      "loss train: 0.09672355570980994 acc train: 0.958631662688942\n",
      "loss test: 0.14967014588891514 acc test: 0.8481481481481481\n",
      "loss train: 0.09546813816161352 acc train: 0.9618138424821002\n",
      "loss test: 0.14900392317245645 acc test: 0.8481481481481481\n",
      "loss train: 0.09424612943881867 acc train: 0.9626093874303898\n",
      "loss test: 0.1483575213064651 acc test: 0.8462962962962963\n",
      "loss train: 0.09304779444409858 acc train: 0.9634049323786794\n",
      "loss test: 0.14774077149938614 acc test: 0.8462962962962963\n",
      "loss train: 0.09187327277240524 acc train: 0.9634049323786794\n",
      "loss test: 0.1471598147550781 acc test: 0.8462962962962963\n",
      "loss train: 0.09073120556654599 acc train: 0.9665871121718377\n",
      "loss test: 0.1466104050449806 acc test: 0.8462962962962963\n",
      "loss train: 0.0896244715716244 acc train: 0.9673826571201273\n",
      "loss test: 0.1460931990023737 acc test: 0.8462962962962963\n",
      "loss train: 0.08854338669843767 acc train: 0.9689737470167065\n",
      "loss test: 0.1456036899524003 acc test: 0.8462962962962963\n",
      "loss train: 0.08748253286304321 acc train: 0.9689737470167065\n",
      "loss test: 0.1451260185148338 acc test: 0.8518518518518519\n",
      "loss train: 0.08647735418825495 acc train: 0.9697692919649961\n",
      "loss test: 0.14465964220721167 acc test: 0.8518518518518519\n",
      "loss train: 0.08552292861780246 acc train: 0.9705648369132857\n",
      "loss test: 0.14420888518576538 acc test: 0.8537037037037037\n",
      "loss train: 0.08460254190165087 acc train: 0.9705648369132857\n",
      "loss test: 0.14377306713577895 acc test: 0.8555555555555555\n",
      "loss train: 0.08370886142455716 acc train: 0.9713603818615751\n",
      "loss test: 0.14335002384598672 acc test: 0.8555555555555555\n",
      "loss train: 0.08283795326404271 acc train: 0.9737470167064439\n",
      "loss test: 0.1429379419054995 acc test: 0.8574074074074074\n",
      "loss train: 0.08198618474764348 acc train: 0.9761336515513126\n",
      "loss test: 0.14253569739963529 acc test: 0.8574074074074074\n",
      "loss train: 0.08114881897741724 acc train: 0.9769291964996022\n",
      "loss test: 0.14214270294418427 acc test: 0.8592592592592593\n",
      "loss train: 0.08032076266020544 acc train: 0.9769291964996022\n",
      "loss test: 0.14175943381433068 acc test: 0.8592592592592593\n",
      "loss train: 0.07950057312911495 acc train: 0.9769291964996022\n",
      "loss test: 0.14138643153294042 acc test: 0.8592592592592593\n",
      "loss train: 0.07868884092973832 acc train: 0.9769291964996022\n",
      "loss test: 0.1410230121230536 acc test: 0.8611111111111112\n",
      "loss train: 0.07788242512916288 acc train: 0.9769291964996022\n",
      "loss test: 0.14066467219350243 acc test: 0.8629629629629629\n",
      "loss train: 0.07708361506962852 acc train: 0.9769291964996022\n",
      "loss test: 0.14030342829302944 acc test: 0.8648148148148148\n",
      "loss train: 0.07630692529366222 acc train: 0.9761336515513126\n",
      "loss test: 0.1399437317430236 acc test: 0.8648148148148148\n",
      "loss train: 0.07556573721875466 acc train: 0.9777247414478918\n",
      "loss test: 0.13960040217986788 acc test: 0.8666666666666667\n",
      "loss train: 0.07483641865973602 acc train: 0.9785202863961814\n",
      "loss test: 0.13927299148348493 acc test: 0.8685185185185185\n",
      "loss train: 0.07360628186984254 acc train: 0.9809069212410502\n",
      "loss test: 0.13876888663360357 acc test: 0.8759259259259259\n",
      "loss train: 0.0726212789354969 acc train: 0.9817024661893397\n",
      "loss test: 0.13825615515284043 acc test: 0.8759259259259259\n",
      "loss train: 0.07177190467387423 acc train: 0.9832935560859188\n",
      "loss test: 0.1377906049407381 acc test: 0.8759259259259259\n",
      "loss train: 0.07096742768258626 acc train: 0.9840891010342084\n",
      "loss test: 0.13737425152411217 acc test: 0.8777777777777778\n",
      "loss train: 0.07021738034319487 acc train: 0.9840891010342084\n",
      "loss test: 0.1370150120659364 acc test: 0.8796296296296297\n",
      "loss train: 0.06951191693483172 acc train: 0.9840891010342084\n",
      "loss test: 0.13668770328438598 acc test: 0.8796296296296297\n",
      "loss train: 0.06883616302235357 acc train: 0.9856801909307876\n",
      "loss test: 0.13637304748641718 acc test: 0.8796296296296297\n",
      "loss train: 0.06818477152563296 acc train: 0.9856801909307876\n",
      "loss test: 0.1360628055966283 acc test: 0.8796296296296297\n",
      "loss train: 0.06755368694400288 acc train: 0.9856801909307876\n",
      "loss test: 0.1357567946907098 acc test: 0.8796296296296297\n",
      "loss train: 0.06693932399201193 acc train: 0.9856801909307876\n",
      "loss test: 0.13545359902066226 acc test: 0.8796296296296297\n",
      "loss train: 0.06633991056451749 acc train: 0.9856801909307876\n",
      "loss test: 0.13514996974632973 acc test: 0.8796296296296297\n",
      "loss train: 0.0657545978468992 acc train: 0.9856801909307876\n",
      "loss test: 0.13484397047811747 acc test: 0.8796296296296297\n",
      "loss train: 0.0651822420785309 acc train: 0.9856801909307876\n",
      "loss test: 0.13453558853767653 acc test: 0.8796296296296297\n",
      "loss train: 0.06462084681108024 acc train: 0.9856801909307876\n",
      "loss test: 0.13422627915264373 acc test: 0.8796296296296297\n",
      "loss train: 0.0640667727344219 acc train: 0.9856801909307876\n",
      "loss test: 0.13391930510938188 acc test: 0.8796296296296297\n",
      "loss train: 0.06351211276668423 acc train: 0.984884645982498\n",
      "loss test: 0.1336205760271897 acc test: 0.8814814814814815\n",
      "loss train: 0.06294200724927014 acc train: 0.984884645982498\n",
      "loss test: 0.1333370456948091 acc test: 0.8814814814814815\n",
      "loss train: 0.062360412709614434 acc train: 0.984884645982498\n",
      "loss test: 0.1330710403949063 acc test: 0.8814814814814815\n",
      "loss train: 0.061808092382552596 acc train: 0.9856801909307876\n",
      "loss test: 0.13282092664003348 acc test: 0.8814814814814815\n",
      "loss train: 0.061286359342895794 acc train: 0.9864757358790772\n",
      "loss test: 0.13258340229316032 acc test: 0.8814814814814815\n",
      "loss train: 0.060782914754395234 acc train: 0.9864757358790772\n",
      "loss test: 0.132355482862536 acc test: 0.8814814814814815\n",
      "loss train: 0.060293580881435715 acc train: 0.9864757358790772\n",
      "loss test: 0.1321349161697808 acc test: 0.8814814814814815\n",
      "loss train: 0.05981609352944658 acc train: 0.9872712808273667\n",
      "loss test: 0.13192040263820284 acc test: 0.8796296296296297\n",
      "loss train: 0.05934829950976346 acc train: 0.9880668257756563\n",
      "loss test: 0.13171130599997632 acc test: 0.8814814814814815\n",
      "loss train: 0.05888895118950163 acc train: 0.9880668257756563\n",
      "loss test: 0.13150659307330384 acc test: 0.8814814814814815\n",
      "loss train: 0.05843898483262697 acc train: 0.9880668257756563\n",
      "loss test: 0.13130497309521025 acc test: 0.8814814814814815\n",
      "loss train: 0.05800000872714994 acc train: 0.9880668257756563\n",
      "loss test: 0.13110769308475334 acc test: 0.8814814814814815\n",
      "loss train: 0.057571090156390045 acc train: 0.9880668257756563\n",
      "loss test: 0.1309173276799084 acc test: 0.8833333333333333\n",
      "loss train: 0.05715001988835605 acc train: 0.9880668257756563\n",
      "loss test: 0.1307346379996273 acc test: 0.8851851851851852\n",
      "loss train: 0.05673545135576867 acc train: 0.9880668257756563\n",
      "loss test: 0.1305595991200651 acc test: 0.8851851851851852\n",
      "loss train: 0.056326923312091104 acc train: 0.9880668257756563\n",
      "loss test: 0.13039214717621056 acc test: 0.8870370370370371\n",
      "loss train: 0.05592448285119649 acc train: 0.9888623707239459\n",
      "loss test: 0.1302319960184667 acc test: 0.8870370370370371\n",
      "loss train: 0.055528438148849654 acc train: 0.9896579156722355\n",
      "loss test: 0.1300784945416507 acc test: 0.8870370370370371\n",
      "loss train: 0.05513917040993079 acc train: 0.9896579156722355\n",
      "loss test: 0.1299307131046507 acc test: 0.8870370370370371\n",
      "loss train: 0.05475703031274384 acc train: 0.9896579156722355\n",
      "loss test: 0.12978762730385845 acc test: 0.8870370370370371\n",
      "loss train: 0.05438231215522192 acc train: 0.9896579156722355\n",
      "loss test: 0.12964823581623366 acc test: 0.8870370370370371\n",
      "loss train: 0.05401522096110987 acc train: 0.9912490055688147\n",
      "loss test: 0.1295116045603649 acc test: 0.8870370370370371\n",
      "loss train: 0.05365578545817365 acc train: 0.9912490055688147\n",
      "loss test: 0.1293769339197677 acc test: 0.8888888888888888\n",
      "loss train: 0.053303783559923386 acc train: 0.9912490055688147\n",
      "loss test: 0.12924366031196324 acc test: 0.8888888888888888\n",
      "loss train: 0.052958755416559006 acc train: 0.9912490055688147\n",
      "loss test: 0.12911151097654844 acc test: 0.8888888888888888\n",
      "loss train: 0.05262007697789935 acc train: 0.9912490055688147\n",
      "loss test: 0.12898047391761852 acc test: 0.8888888888888888\n",
      "loss train: 0.05228702311876366 acc train: 0.9912490055688147\n",
      "loss test: 0.1288507184279314 acc test: 0.8870370370370371\n",
      "loss train: 0.05195878971681433 acc train: 0.9912490055688147\n",
      "loss test: 0.12872251359355627 acc test: 0.8888888888888888\n",
      "loss train: 0.05163447899405569 acc train: 0.9912490055688147\n",
      "loss test: 0.12859616937818708 acc test: 0.8888888888888888\n",
      "loss train: 0.051313053366033444 acc train: 0.9912490055688147\n",
      "loss test: 0.12847200637319925 acc test: 0.8888888888888888\n",
      "loss train: 0.05099324882827933 acc train: 0.9920445505171042\n",
      "loss test: 0.1283503550176857 acc test: 0.8888888888888888\n",
      "loss train: 0.050673420518954064 acc train: 0.9920445505171042\n",
      "loss test: 0.1282315900647353 acc test: 0.8888888888888888\n",
      "loss train: 0.05035127556123182 acc train: 0.9928400954653938\n",
      "loss test: 0.12811621781197952 acc test: 0.8925925925925926\n",
      "loss train: 0.05002347876923372 acc train: 0.9928400954653938\n",
      "loss test: 0.12800503331293556 acc test: 0.8925925925925926\n",
      "loss train: 0.04968543186947461 acc train: 0.9928400954653938\n",
      "loss test: 0.12789922489956218 acc test: 0.8925925925925926\n",
      "loss train: 0.04933277767717454 acc train: 0.9928400954653938\n",
      "loss test: 0.12779974808526176 acc test: 0.8925925925925926\n",
      "loss train: 0.04896710206401945 acc train: 0.9928400954653938\n",
      "loss test: 0.1277058106590575 acc test: 0.8907407407407407\n",
      "loss train: 0.04859812825913459 acc train: 0.9928400954653938\n",
      "loss test: 0.12761563950429658 acc test: 0.8944444444444445\n",
      "loss train: 0.04823373550711521 acc train: 0.9928400954653938\n",
      "loss test: 0.12752819096235166 acc test: 0.8944444444444445\n",
      "loss train: 0.04787676324157242 acc train: 0.9928400954653938\n",
      "loss test: 0.12744217287215046 acc test: 0.8944444444444445\n",
      "loss train: 0.047526434736217565 acc train: 0.9928400954653938\n",
      "loss test: 0.1273567531471768 acc test: 0.8944444444444445\n",
      "loss train: 0.047180920420890525 acc train: 0.9936356404136834\n",
      "loss test: 0.12727318401884766 acc test: 0.8944444444444445\n",
      "loss train: 0.04683912433437633 acc train: 0.9936356404136834\n",
      "loss test: 0.12719238024170287 acc test: 0.8925925925925926\n",
      "loss train: 0.04650079480503079 acc train: 0.9936356404136834\n",
      "loss test: 0.12711190347559778 acc test: 0.8925925925925926\n",
      "loss train: 0.046167608874107945 acc train: 0.994431185361973\n",
      "loss test: 0.1270274541170517 acc test: 0.8925925925925926\n",
      "loss train: 0.045843498584898945 acc train: 0.994431185361973\n",
      "loss test: 0.12693690241872285 acc test: 0.8925925925925926\n",
      "loss train: 0.04553085909538974 acc train: 0.994431185361973\n",
      "loss test: 0.1268412597205962 acc test: 0.8944444444444445\n",
      "loss train: 0.045228005374588834 acc train: 0.994431185361973\n",
      "loss test: 0.126742473831454 acc test: 0.8944444444444445\n",
      "loss train: 0.04493236077177094 acc train: 0.994431185361973\n",
      "loss test: 0.12664202676666736 acc test: 0.8944444444444445\n",
      "loss train: 0.044642402324356095 acc train: 0.994431185361973\n",
      "loss test: 0.1265407131363024 acc test: 0.8944444444444445\n",
      "loss train: 0.04435742278755973 acc train: 0.994431185361973\n",
      "loss test: 0.12643891630387213 acc test: 0.8944444444444445\n",
      "loss train: 0.04407689324550124 acc train: 0.994431185361973\n",
      "loss test: 0.12633701360651095 acc test: 0.8944444444444445\n",
      "loss train: 0.04379985052831169 acc train: 0.994431185361973\n",
      "loss test: 0.12623571991259477 acc test: 0.8944444444444445\n",
      "loss train: 0.043524259004741904 acc train: 0.994431185361973\n",
      "loss test: 0.1261364675600658 acc test: 0.8944444444444445\n",
      "loss train: 0.043245603046596186 acc train: 0.994431185361973\n",
      "loss test: 0.12604242715104558 acc test: 0.8944444444444445\n",
      "loss train: 0.042951841358470966 acc train: 0.994431185361973\n",
      "loss test: 0.1259606678972848 acc test: 0.8944444444444445\n",
      "loss train: 0.0426081423541024 acc train: 0.994431185361973\n",
      "loss test: 0.125894694972405 acc test: 0.8944444444444445\n",
      "loss train: 0.042208177439277905 acc train: 0.994431185361973\n",
      "loss test: 0.12581890593378525 acc test: 0.8944444444444445\n",
      "loss train: 0.04190484929367802 acc train: 0.994431185361973\n",
      "loss test: 0.12573101703868306 acc test: 0.8925925925925926\n",
      "loss train: 0.04163557032174392 acc train: 0.994431185361973\n",
      "loss test: 0.12564053166674452 acc test: 0.8925925925925926\n",
      "loss train: 0.041371961787489434 acc train: 0.9952267303102625\n",
      "loss test: 0.1255523482130357 acc test: 0.8925925925925926\n",
      "loss train: 0.041110374610678714 acc train: 0.9952267303102625\n",
      "loss test: 0.12546820397965677 acc test: 0.8944444444444445\n",
      "loss train: 0.04084840612781836 acc train: 0.9960222752585521\n",
      "loss test: 0.125389411060378 acc test: 0.8944444444444445\n",
      "loss train: 0.04058367144851702 acc train: 0.9960222752585521\n",
      "loss test: 0.12531697271690345 acc test: 0.8962962962962963\n",
      "loss train: 0.04031435006448853 acc train: 0.9960222752585521\n",
      "loss test: 0.12525071385703107 acc test: 0.8962962962962963\n",
      "loss train: 0.04004113010850096 acc train: 0.9960222752585521\n",
      "loss test: 0.1251885249314806 acc test: 0.8962962962962963\n",
      "loss train: 0.03976885598942301 acc train: 0.9960222752585521\n",
      "loss test: 0.12512737377313873 acc test: 0.8962962962962963\n",
      "loss train: 0.03950346935159867 acc train: 0.9968178202068417\n",
      "loss test: 0.12506543752495253 acc test: 0.8962962962962963\n",
      "loss train: 0.0392468677780116 acc train: 0.9968178202068417\n",
      "loss test: 0.12500248424221877 acc test: 0.8962962962962963\n",
      "loss train: 0.038997137187983144 acc train: 0.9968178202068417\n",
      "loss test: 0.12493886325718268 acc test: 0.8981481481481481\n",
      "loss train: 0.038751390540789535 acc train: 0.9968178202068417\n",
      "loss test: 0.12487489070870293 acc test: 0.8981481481481481\n",
      "loss train: 0.03850741850424133 acc train: 0.9968178202068417\n",
      "loss test: 0.12481083507082158 acc test: 0.8981481481481481\n",
      "loss train: 0.03826476277276331 acc train: 0.9968178202068417\n",
      "loss test: 0.12474729597614324 acc test: 0.8981481481481481\n",
      "loss train: 0.038025045337650755 acc train: 0.9968178202068417\n",
      "loss test: 0.12468528251761567 acc test: 0.8981481481481481\n",
      "loss train: 0.03779000689398871 acc train: 0.9968178202068417\n",
      "loss test: 0.12462521085280058 acc test: 0.8981481481481481\n",
      "loss train: 0.03755955808835641 acc train: 0.9968178202068417\n",
      "loss test: 0.1245665070763298 acc test: 0.8981481481481481\n",
      "loss train: 0.037332668394494554 acc train: 0.9968178202068417\n",
      "loss test: 0.12450861594208626 acc test: 0.8981481481481481\n",
      "loss train: 0.03710833434681486 acc train: 0.9968178202068417\n",
      "loss test: 0.12445143578180286 acc test: 0.8981481481481481\n",
      "loss train: 0.03688558174886664 acc train: 0.9968178202068417\n",
      "loss test: 0.12439495505334315 acc test: 0.8981481481481481\n",
      "loss train: 0.03666361452753456 acc train: 0.9968178202068417\n",
      "loss test: 0.12433888690723437 acc test: 0.8981481481481481\n",
      "loss train: 0.03644251042632609 acc train: 0.9968178202068417\n",
      "loss test: 0.1242826409359473 acc test: 0.8962962962962963\n",
      "loss train: 0.03622391598486744 acc train: 0.9968178202068417\n",
      "loss test: 0.12422574580624056 acc test: 0.8962962962962963\n",
      "loss train: 0.036009664242735245 acc train: 0.9968178202068417\n",
      "loss test: 0.1241683616683545 acc test: 0.8962962962962963\n",
      "loss train: 0.03579923289570376 acc train: 0.9968178202068417\n",
      "loss test: 0.12411131378631768 acc test: 0.8962962962962963\n",
      "loss train: 0.03559058210970773 acc train: 0.9968178202068417\n",
      "loss test: 0.12405552925937358 acc test: 0.8962962962962963\n",
      "loss train: 0.03538188947283753 acc train: 0.9968178202068417\n",
      "loss test: 0.12400158609862456 acc test: 0.8962962962962963\n",
      "loss train: 0.03517177184141018 acc train: 0.9968178202068417\n",
      "loss test: 0.12394972013846266 acc test: 0.8962962962962963\n",
      "loss train: 0.03495921889986976 acc train: 0.9968178202068417\n",
      "loss test: 0.1238999209752957 acc test: 0.8981481481481481\n",
      "loss train: 0.03474385488515655 acc train: 0.9968178202068417\n",
      "loss test: 0.12385198886506335 acc test: 0.8981481481481481\n",
      "loss train: 0.0345264013683566 acc train: 0.9968178202068417\n",
      "loss test: 0.1238056001552852 acc test: 0.8981481481481481\n",
      "loss train: 0.03430867341080203 acc train: 0.9968178202068417\n",
      "loss test: 0.12376032395736222 acc test: 0.8981481481481481\n",
      "loss train: 0.03409264484445271 acc train: 0.9976133651551312\n",
      "loss test: 0.12371557228449169 acc test: 0.8981481481481481\n",
      "loss train: 0.03387949917489646 acc train: 0.9976133651551312\n",
      "loss test: 0.12367069642539794 acc test: 0.8981481481481481\n",
      "loss train: 0.03366961765862139 acc train: 0.9976133651551312\n",
      "loss test: 0.12362522192423604 acc test: 0.8981481481481481\n",
      "loss train: 0.03346300746220889 acc train: 0.9976133651551312\n",
      "loss test: 0.123579002915323 acc test: 0.8981481481481481\n",
      "loss train: 0.03325958046424778 acc train: 0.9976133651551312\n",
      "loss test: 0.123532238701395 acc test: 0.8981481481481481\n",
      "loss train: 0.033059267156869916 acc train: 0.9976133651551312\n",
      "loss test: 0.12348538136196831 acc test: 0.8981481481481481\n",
      "loss train: 0.032862047244566484 acc train: 0.9976133651551312\n",
      "loss test: 0.12343898312161065 acc test: 0.8981481481481481\n",
      "loss train: 0.032667931456917494 acc train: 0.9976133651551312\n",
      "loss test: 0.1233935518538966 acc test: 0.8981481481481481\n",
      "loss train: 0.032476924874615204 acc train: 0.9976133651551312\n",
      "loss test: 0.12334946243392293 acc test: 0.8981481481481481\n",
      "loss train: 0.03228899836554228 acc train: 0.9976133651551312\n",
      "loss test: 0.12330693067982161 acc test: 0.9\n",
      "loss train: 0.03210407826121053 acc train: 0.9976133651551312\n",
      "loss test: 0.12326603181583695 acc test: 0.9\n",
      "loss train: 0.03192205050397163 acc train: 0.9976133651551312\n",
      "loss test: 0.12322674341867482 acc test: 0.9\n",
      "loss train: 0.03174277189878502 acc train: 0.9976133651551312\n",
      "loss test: 0.12318899850327862 acc test: 0.9\n",
      "loss train: 0.031566083265976715 acc train: 0.9976133651551312\n",
      "loss test: 0.12315273666664613 acc test: 0.9\n",
      "loss train: 0.03139182216878926 acc train: 0.9976133651551312\n",
      "loss test: 0.12311794100015604 acc test: 0.9\n",
      "loss train: 0.03121983510655447 acc train: 0.9976133651551312\n",
      "loss test: 0.1230846489212973 acc test: 0.9\n",
      "loss train: 0.03104998987891915 acc train: 0.9976133651551312\n",
      "loss test: 0.12305292700471065 acc test: 0.9\n",
      "loss train: 0.030882187231764086 acc train: 0.9976133651551312\n",
      "loss test: 0.12302280698857287 acc test: 0.9\n",
      "loss train: 0.03071636707968305 acc train: 0.9984089101034208\n",
      "loss test: 0.12299419967293962 acc test: 0.9\n",
      "loss train: 0.030552501462221878 acc train: 0.9984089101034208\n",
      "loss test: 0.12296683220250021 acc test: 0.9\n",
      "loss train: 0.03039056928629027 acc train: 0.9984089101034208\n",
      "loss test: 0.12294026223486337 acc test: 0.9\n",
      "loss train: 0.030230518768296646 acc train: 0.9984089101034208\n",
      "loss test: 0.12291397775413124 acc test: 0.9\n",
      "loss train: 0.030072233295262027 acc train: 0.9984089101034208\n",
      "loss test: 0.12288752262062347 acc test: 0.8981481481481481\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "MLPstructure = edict(\n",
    "    D_in = X_train.shape[1],\n",
    "    H1 = 128,\n",
    "    H2 = 32,\n",
    "    D_out = Y_train.shape[1],\n",
    "    η = 0.001,\n",
    ")\n",
    "\n",
    "model = MLP(MLPstructure)\n",
    "train_output = model.fit(X_train, Y_train, X_test, Y_test, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W1, W2, W3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(D_in, H1), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(H1, H2), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(H2, D_out)\n\u001b[0;32m      2\u001b[0m B1, B2, B3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(H1), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(H2), np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(D_out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'D_in' is not defined"
     ]
    }
   ],
   "source": [
    "W1, W2, W3 = np.random.randn(D_in, H1), np.random.randn(H1, H2), np.random.randn(H2, D_out)\n",
    "B1, B2, B3 = np.random.randn(H1), np.random.randn(H2), np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 0.32876991970936065 acc train: 0.15910898965791567\n",
      "loss test: 0.2974473791285755 acc test: 0.27037037037037037\n",
      "loss train: 0.28861985874357066 acc train: 0.3015115354017502\n",
      "loss test: 0.27764417575961214 acc test: 0.3851851851851852\n",
      "loss train: 0.26766636073440314 acc train: 0.4303898170246619\n",
      "loss test: 0.2668043004484215 acc test: 0.4462962962962963\n",
      "loss train: 0.2512767568865605 acc train: 0.5330151153540175\n",
      "loss test: 0.2543257343808141 acc test: 0.512962962962963\n",
      "loss train: 0.2360286431243093 acc train: 0.6062052505966588\n",
      "loss test: 0.24101387070737132 acc test: 0.575925925925926\n",
      "loss train: 0.22214937668116133 acc train: 0.6706443914081146\n",
      "loss test: 0.23044991209608143 acc test: 0.6240740740740741\n",
      "loss train: 0.2112238768078817 acc train: 0.7128082736674622\n",
      "loss test: 0.22261298877347147 acc test: 0.6555555555555556\n",
      "loss train: 0.20164746765740355 acc train: 0.7597454256165473\n",
      "loss test: 0.2160182371619532 acc test: 0.6777777777777778\n",
      "loss train: 0.19311878286327966 acc train: 0.7875894988066826\n",
      "loss test: 0.21088572520778515 acc test: 0.7\n",
      "loss train: 0.1855809841801524 acc train: 0.807478122513922\n",
      "loss test: 0.2056588176887253 acc test: 0.7203703703703703\n",
      "loss train: 0.17852077990572693 acc train: 0.8194112967382657\n",
      "loss test: 0.20085766569172414 acc test: 0.7333333333333333\n",
      "loss train: 0.1719676936161999 acc train: 0.8369132856006364\n",
      "loss test: 0.19641454601489486 acc test: 0.7481481481481481\n",
      "loss train: 0.16606464039622898 acc train: 0.8544152744630071\n",
      "loss test: 0.19220707526856598 acc test: 0.7648148148148148\n",
      "loss train: 0.16075425101802998 acc train: 0.8591885441527446\n",
      "loss test: 0.18841882782238573 acc test: 0.774074074074074\n",
      "loss train: 0.15586992397682956 acc train: 0.8671439936356404\n",
      "loss test: 0.185006713920429 acc test: 0.7888888888888889\n",
      "loss train: 0.151353662722296 acc train: 0.8798727128082736\n",
      "loss test: 0.18184297803879734 acc test: 0.7907407407407407\n",
      "loss train: 0.14704747278303623 acc train: 0.8870326173428799\n",
      "loss test: 0.1788919506455945 acc test: 0.7962962962962963\n",
      "loss train: 0.14287848038279705 acc train: 0.8933969769291965\n",
      "loss test: 0.17615208849589462 acc test: 0.8018518518518518\n",
      "loss train: 0.13893029049514583 acc train: 0.9005568814638027\n",
      "loss test: 0.17352046210654454 acc test: 0.8055555555555556\n",
      "loss train: 0.13528129038359793 acc train: 0.903739061256961\n",
      "loss test: 0.17108737026163573 acc test: 0.8111111111111111\n",
      "loss train: 0.13186825078745545 acc train: 0.9140811455847255\n",
      "loss test: 0.16880563064585014 acc test: 0.8185185185185185\n",
      "loss train: 0.128650453724035 acc train: 0.9164677804295943\n",
      "loss test: 0.16662308337829054 acc test: 0.8185185185185185\n",
      "loss train: 0.12560094526994678 acc train: 0.9236276849642004\n",
      "loss test: 0.1645012097475306 acc test: 0.8203703703703704\n",
      "loss train: 0.12263944419599108 acc train: 0.9284009546539379\n",
      "loss test: 0.1624577356316114 acc test: 0.8203703703703704\n",
      "loss train: 0.11971004058099592 acc train: 0.9347653142402546\n",
      "loss test: 0.16055838361351782 acc test: 0.825925925925926\n",
      "loss train: 0.11693812675615324 acc train: 0.9371519490851233\n",
      "loss test: 0.15892328739752123 acc test: 0.8314814814814815\n",
      "loss train: 0.11429368898967568 acc train: 0.9411296738265712\n",
      "loss test: 0.15746095212349115 acc test: 0.8351851851851851\n",
      "loss train: 0.11173628541998872 acc train: 0.9443118536197295\n",
      "loss test: 0.1561170646796053 acc test: 0.8407407407407408\n",
      "loss train: 0.1092498685782916 acc train: 0.9474940334128878\n",
      "loss test: 0.15486729624078416 acc test: 0.8444444444444444\n",
      "loss train: 0.10687768505782923 acc train: 0.9506762132060461\n",
      "loss test: 0.1536586630365175 acc test: 0.8462962962962963\n",
      "loss train: 0.10463288773472555 acc train: 0.9538583929992045\n",
      "loss test: 0.15248158695827121 acc test: 0.8537037037037037\n",
      "loss train: 0.10251299690967643 acc train: 0.954653937947494\n",
      "loss test: 0.15134105123962818 acc test: 0.85\n",
      "loss train: 0.10050899774083633 acc train: 0.9578361177406524\n",
      "loss test: 0.15025484986999155 acc test: 0.85\n",
      "loss train: 0.09860827738642856 acc train: 0.9594272076372315\n",
      "loss test: 0.14925744527245835 acc test: 0.8537037037037037\n",
      "loss train: 0.09685144950216602 acc train: 0.960222752585521\n",
      "loss test: 0.14831900258817773 acc test: 0.8574074074074074\n",
      "loss train: 0.09517896677954282 acc train: 0.9610182975338106\n",
      "loss test: 0.14741975002266844 acc test: 0.8592592592592593\n",
      "loss train: 0.09356821061930555 acc train: 0.9634049323786794\n",
      "loss test: 0.14655566396791767 acc test: 0.8592592592592593\n",
      "loss train: 0.0920067410128194 acc train: 0.9626093874303898\n",
      "loss test: 0.14572486018455014 acc test: 0.8592592592592593\n",
      "loss train: 0.09049197866531056 acc train: 0.9649960222752586\n",
      "loss test: 0.1449269782316547 acc test: 0.8629629629629629\n",
      "loss train: 0.0890299723648067 acc train: 0.9649960222752586\n",
      "loss test: 0.1441606367650201 acc test: 0.8666666666666667\n",
      "loss train: 0.08761658055146786 acc train: 0.9665871121718377\n",
      "loss test: 0.14341498862185686 acc test: 0.8666666666666667\n",
      "loss train: 0.086233859056692 acc train: 0.9665871121718377\n",
      "loss test: 0.1426792027536581 acc test: 0.8722222222222222\n",
      "loss train: 0.08486612496143894 acc train: 0.9689737470167065\n",
      "loss test: 0.14194779818510744 acc test: 0.8722222222222222\n",
      "loss train: 0.08350416825208437 acc train: 0.9713603818615751\n",
      "loss test: 0.1412345578354642 acc test: 0.8740740740740741\n",
      "loss train: 0.08215547499813615 acc train: 0.9721559268098647\n",
      "loss test: 0.14058199248214573 acc test: 0.8740740740740741\n",
      "loss train: 0.0808410018099515 acc train: 0.9729514717581543\n",
      "loss test: 0.14001032637454697 acc test: 0.8759259259259259\n",
      "loss train: 0.0795600889862264 acc train: 0.9737470167064439\n",
      "loss test: 0.13949468518244026 acc test: 0.8777777777777778\n",
      "loss train: 0.07831453261493862 acc train: 0.9745425616547335\n",
      "loss test: 0.13900508162212358 acc test: 0.8777777777777778\n",
      "loss train: 0.0771082006322052 acc train: 0.9761336515513126\n",
      "loss test: 0.13852271371170138 acc test: 0.8777777777777778\n",
      "loss train: 0.07593072273354136 acc train: 0.9785202863961814\n",
      "loss test: 0.13804037251054804 acc test: 0.8777777777777778\n",
      "loss train: 0.07479063998476702 acc train: 0.9785202863961814\n",
      "loss test: 0.13756375454227707 acc test: 0.8796296296296297\n",
      "loss train: 0.07370326458564339 acc train: 0.979315831344471\n",
      "loss test: 0.13710561156160153 acc test: 0.8814814814814815\n",
      "loss train: 0.07265950270045035 acc train: 0.979315831344471\n",
      "loss test: 0.13668412562235868 acc test: 0.8833333333333333\n",
      "loss train: 0.07164533977039403 acc train: 0.9801113762927606\n",
      "loss test: 0.13633028689128898 acc test: 0.8851851851851852\n",
      "loss train: 0.07066525762432611 acc train: 0.9809069212410502\n",
      "loss test: 0.13601201825911566 acc test: 0.8851851851851852\n",
      "loss train: 0.06971521759697523 acc train: 0.9817024661893397\n",
      "loss test: 0.13560951608188573 acc test: 0.8851851851851852\n",
      "loss train: 0.06872947918078653 acc train: 0.9832935560859188\n",
      "loss test: 0.13520508872817164 acc test: 0.8888888888888888\n",
      "loss train: 0.06781523989724411 acc train: 0.9840891010342084\n",
      "loss test: 0.1348065526772569 acc test: 0.8888888888888888\n",
      "loss train: 0.06694784907880534 acc train: 0.984884645982498\n",
      "loss test: 0.13440906082668974 acc test: 0.8907407407407407\n",
      "loss train: 0.06610971776169451 acc train: 0.9864757358790772\n",
      "loss test: 0.13401107208708649 acc test: 0.8925925925925926\n",
      "loss train: 0.06529144084012528 acc train: 0.9872712808273667\n",
      "loss test: 0.1336123798004497 acc test: 0.8925925925925926\n",
      "loss train: 0.06448975703346836 acc train: 0.9880668257756563\n",
      "loss test: 0.13321443509286798 acc test: 0.8925925925925926\n",
      "loss train: 0.06370908349483874 acc train: 0.9880668257756563\n",
      "loss test: 0.1328208315560036 acc test: 0.8944444444444445\n",
      "loss train: 0.06295351535534831 acc train: 0.9888623707239459\n",
      "loss test: 0.13243481114384192 acc test: 0.8944444444444445\n",
      "loss train: 0.0622197806431561 acc train: 0.9896579156722355\n",
      "loss test: 0.13205690992391025 acc test: 0.8944444444444445\n",
      "loss train: 0.06150196811496782 acc train: 0.9896579156722355\n",
      "loss test: 0.13168607054119627 acc test: 0.8944444444444445\n",
      "loss train: 0.06079329670024572 acc train: 0.9904534606205251\n",
      "loss test: 0.13132220364775185 acc test: 0.8944444444444445\n",
      "loss train: 0.060080835725636814 acc train: 0.9904534606205251\n",
      "loss test: 0.13096904821356212 acc test: 0.8962962962962963\n",
      "loss train: 0.059328255554225814 acc train: 0.9904534606205251\n",
      "loss test: 0.13063280099059485 acc test: 0.8981481481481481\n",
      "loss train: 0.05853070705785464 acc train: 0.9904534606205251\n",
      "loss test: 0.13030337043205456 acc test: 0.8981481481481481\n",
      "loss train: 0.057839941551876245 acc train: 0.9912490055688147\n",
      "loss test: 0.12998564210428548 acc test: 0.8981481481481481\n",
      "loss train: 0.057201181088971625 acc train: 0.9912490055688147\n",
      "loss test: 0.1296801864054036 acc test: 0.8981481481481481\n",
      "loss train: 0.05658587110009373 acc train: 0.9912490055688147\n",
      "loss test: 0.1293838784756752 acc test: 0.8981481481481481\n",
      "loss train: 0.05598987496262132 acc train: 0.9912490055688147\n",
      "loss test: 0.12909405562548915 acc test: 0.8962962962962963\n",
      "loss train: 0.055411333027526447 acc train: 0.9912490055688147\n",
      "loss test: 0.12880926079051938 acc test: 0.8962962962962963\n",
      "loss train: 0.05484859030496066 acc train: 0.9912490055688147\n",
      "loss test: 0.12852902770232685 acc test: 0.8944444444444445\n",
      "loss train: 0.05430003895301808 acc train: 0.9912490055688147\n",
      "loss test: 0.12825354717813758 acc test: 0.8962962962962963\n",
      "loss train: 0.05376430704189533 acc train: 0.9912490055688147\n",
      "loss test: 0.12798347873603613 acc test: 0.8962962962962963\n",
      "loss train: 0.05324040633628905 acc train: 0.9912490055688147\n",
      "loss test: 0.12771976571103313 acc test: 0.8962962962962963\n",
      "loss train: 0.05272766783947665 acc train: 0.9912490055688147\n",
      "loss test: 0.12746323464004827 acc test: 0.8962962962962963\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    # train\n",
    "    \n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "        # back propagation\n",
    "\n",
    "        # layer 3\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_W3 = out2.T @ error\n",
    "        grad_B3 = error\n",
    "\n",
    "        # layer 2\n",
    "        error = error @ W3.T * out2 * (1 - out2)\n",
    "        grad_W2 = out1.T @ error\n",
    "        grad_B2 = error\n",
    "\n",
    "        # layer 1\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_W1 = x @ error\n",
    "        grad_B1 = error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer 1\n",
    "        W1 = W1 - η * grad_W1\n",
    "        B1 = B1 - η * grad_B1\n",
    "        \n",
    "        # layer 2\n",
    "        W2 = W2 - η * grad_W2\n",
    "        B2 = B2 - η * grad_B2\n",
    "\n",
    "        # layer 3\n",
    "        W3 = W3 - η * grad_W3\n",
    "        B3 = B3 - η * grad_B3\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_train = root_mean_squired_error(Y_pred, Y_train)\n",
    "    acc_train = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_train, axis=1))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_test = root_mean_squired_error(Y_pred, Y_test)\n",
    "    acc_test = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_test, axis=1))\n",
    "\n",
    "    print('loss train:', loss_train, 'acc train:', acc_train)\n",
    "    print('loss test:', loss_test, 'acc test:', acc_test)\n",
    "\n",
    "print('train completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"input/test4.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "x = image.reshape(-1, 1)\n",
    "\n",
    "# layer 1\n",
    "net1 = x.T @ W1 + B1\n",
    "out1 = sigmoid(net1)\n",
    "\n",
    "# layer 2\n",
    "net2 = out1 @ W2 + B2\n",
    "out2 = sigmoid(net2)\n",
    "\n",
    "# layer 3\n",
    "net3 = out2 @ W3 + B3\n",
    "out3 = softmax(net3)\n",
    "\n",
    "y_pred = out3\n",
    "print(np.argmax(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  6., 15., 15., 15., 16.,  4.,  0.],\n",
       "       [ 0., 12., 15.,  2.,  0., 14., 12.,  0.],\n",
       "       [ 0., 13., 14.,  0.,  0., 13., 14.,  0.],\n",
       "       [ 0., 13., 14.,  0.,  0., 13., 14.,  0.],\n",
       "       [ 0., 12., 14.,  0.,  0., 14., 13.,  0.],\n",
       "       [ 0., 11., 15.,  0.,  2., 16., 13.,  0.],\n",
       "       [ 0.,  5., 16.,  6., 13., 16.,  9.,  0.],\n",
       "       [ 0.,  1., 14., 16., 16., 15.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
